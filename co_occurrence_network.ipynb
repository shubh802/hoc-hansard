{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Co-Occurrence Network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+  In this file we will make the co-occurrence network of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from warnings import filterwarnings\n",
    "from collections import Counter\n",
    "from pylab import rcParams\n",
    "filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from lda_topic_modelling.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joshi\\anaconda3\\envs\\thesis\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "c:\\Users\\joshi\\anaconda3\\envs\\thesis\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "c:\\Users\\joshi\\anaconda3\\envs\\thesis\\lib\\site-packages\\gensim\\matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n",
      "c:\\Users\\joshi\\anaconda3\\envs\\thesis\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing another notebook\n",
    "import import_ipynb\n",
    "import lda_topic_modelling as lda\n",
    "\n",
    "# Networkx\n",
    "import networkx as nx\n",
    "\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from functools import reduce\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Co-Occurrence Network and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOCNetwork:\n",
    "    \"\"\"\n",
    "         A class used to create the co-occurence network of the speeches, plot more frequent words in a speech,\n",
    "         plot neighbouring word in the network along with checking the similarity scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = r'./assets/images/'\n",
    "\n",
    "    def get_word_frequency(self, words, n, name):\n",
    "        \"\"\" The function creates plot frequency of word and its count in log form\n",
    "            words - list of words\n",
    "            n - top common words\n",
    "            name - plot name to be saved\"\"\"\n",
    "\n",
    "        # Frequency of most common words\n",
    "        all_fdist = nltk.FreqDist(words).most_common(n)\n",
    "        all_fdist = pd.Series(dict(all_fdist))\n",
    "\n",
    "        # Initailize the subplot\n",
    "        fig, ax = plt.subplots(figsize=(20,10))\n",
    "        # Plotting the words\n",
    "        frpl1 = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax,palette=\"YlOrBr_r\")\n",
    "        plt.xticks(rotation=45);\n",
    "        frpl1.set(title='Frequency Distribution',yscale = 'log')\n",
    "        plt.title(f\"Word Frequency of {name}\", fontsize=35)\n",
    "        plt.xlabel(name, fontsize=16)\n",
    "        plt.ylabel('Counts', fontsize=16)\n",
    "        plt.savefig(HOCNetwork.base_path + name +'.png')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def co_occurence_network(self, list):\n",
    "        \"\"\" The function creates co-occurence network of words using networkx lib and returns the network\n",
    "            list - list of words\"\"\"\n",
    "\n",
    "        # Initialize network   \n",
    "        G = nx.Graph()\n",
    "        for sublist in list:\n",
    "            edlis=[]\n",
    "            for i in range(len(sublist)-1):\n",
    "                edlis.append((sublist[i],sublist[i+1]))\n",
    "        # Adding edges to the network\n",
    "            G.add_edges_from(edlis)\n",
    "        return G\n",
    "\n",
    "    def get_neighbouring_words(self, word, G):\n",
    "        \"\"\" The function calculates the neighbouring words of the word using the co-occurence network.\n",
    "            word - word\n",
    "            G - Network \"\"\"\n",
    "\n",
    "        # Finding the neighbours\n",
    "        neighbour_words = list(G.neighbors(word))\n",
    "        neighbour_words.append(word)\n",
    "        return neighbour_words\n",
    "\n",
    "    def plot_neighbhour_words(self, word, G, neighbour_words):\n",
    "        \"\"\" The function plots the neighbouring words of a network\n",
    "            word - word\n",
    "            G - Network \"\"\"\n",
    "\n",
    "        H=G.subgraph(neighbour_words)\n",
    "        d = dict(H.degree)\n",
    "        plt.figure(3,figsize=(20,10)) \n",
    "        plt.title(f\"Co-Occurence Network of {word} word\", fontsize=35)\n",
    "        nx.draw(H, with_labels=True, font_size=20, node_size=400, node_color='orange')\n",
    "        plt.savefig(HOCNetwork.base_path +word+'.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def deg_closness_word(self, G):\n",
    "        \"\"\" The function calculates the degree and closeness centraility of the network and returns the dataframe of it.\n",
    "            G - Network \"\"\"\n",
    "\n",
    "        ### Degree and its closeness to words\n",
    "        degre_=nx.degree_centrality(G)\n",
    "        clos_=nx.closeness_centrality(G)\n",
    "        data_frame = pd.DataFrame([degre_,clos_]).transpose()\n",
    "        data_frame.columns = [\"Degree\", \"Closeness\"]\n",
    "\n",
    "        # Shows the similarity or the word closer to the selected words\n",
    "        data_frame.sort_values(by=['Closeness'], inplace=True, ascending=False)\n",
    "        return data_frame\n",
    "\n",
    "\n",
    "    def get_reduced_network(self, data_lemmatized):\n",
    "        \"\"\" The function reduces the network based on word frequency in a speech and returns the network\n",
    "            data_lemmatized - lemmatized words\"\"\"\n",
    "\n",
    "        # Filtering words occuring less than 1 times in a document\n",
    "        reduced_list = [[k for k,v in Counter(sublist).items() if v >1] for sublist in data_lemmatized ]\n",
    "\n",
    "        # Creating co-occurence network\n",
    "        G = self.co_occurence_network(reduced_list)\n",
    "        print(f\"No of nodes {G.number_of_nodes()}\")\n",
    "        print(f\"No of edges {G.number_of_edges()}\")\n",
    "        return G\n",
    "\n",
    "\n",
    "    def plot_co_occurence_network(self, G, title_text):\n",
    "        \"\"\" The function used to plot the co-occurence network\n",
    "            G - network\n",
    "            title_text - title of the plot \"\"\"\n",
    "\n",
    "        # Plotting the co-occurence network\n",
    "        plt.figure(3,figsize=(20,14)) \n",
    "        plt.title(title_text, fontsize=25)\n",
    "        nx.draw(G, with_labels=True, font_size=10, node_size=300, node_color='orange')\n",
    "        plt.savefig(HOCNetwork.base_path+title_text+'.png',dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    def check_similarity(self, word_previous, word_current):\n",
    "        \"\"\" The function checks the similarity scores of the two word vectors by finding the similar words in the vector.\n",
    "            word_previous - neighbouring word in previous year\n",
    "            word_current - neighbouring word in current year \"\"\"\n",
    "\n",
    "        # Finding the similar words and their similarity score\n",
    "        common = [ item for item in word_previous if item in word_current]\n",
    "        simalarity_score = len(common)/len(word_previous) * 100\n",
    "        # Return the similarity score\n",
    "        return simalarity_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334928be5ed23319e5e6550927c5d4f3c3b4919c511fd9b77eaf79aad6ef04de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
