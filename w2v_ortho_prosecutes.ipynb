{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Word2Vec Modelling</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from warnings import filterwarnings\n",
    "from collections import Counter\n",
    "from pylab import rcParams\n",
    "filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOCw2v:\n",
    "    \"\"\"\n",
    "         A class to return word2vec model for a given dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, df):\n",
    "        \"\"\"\n",
    "            This function takes a dataframe and returns a list of lists of tokens\n",
    "            df - Dataframe\n",
    "        \"\"\"\n",
    "        df['speech_tokens'] = df.apply(lambda row: word_tokenize(row['speech_processed']), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def word2vec_model(self, data, vector_size=300, min_count=5, workers=4):\n",
    "        \"\"\"\n",
    "            This function takes a dataframe and returns a word2vec model\n",
    "            data - Dataframe\n",
    "            vector_size - Dimension of the vector\n",
    "            min_count - Minimum number of times a word must appear in the corpus to be included in the model\n",
    "            workers - Number of workers to use for training\n",
    "        \"\"\"\n",
    "        model = Word2Vec(data, vector_size=vector_size, min_count=min_count, workers=workers)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOCOrthoPros():\n",
    "    \"\"\"\n",
    "         A class used to measure the semantic change in the vector after performing Embedding space alignment and orthogonal prosecution of the w2v vectors.\n",
    "\n",
    "         Code will be used from https://gist.github.com/zhicongchen/9e23d5c3f1e5b1293b16133485cd17d8, ported from HistWords <https://github.com/williamleif/histwords>.\n",
    "         First, we will define a function to find the intersection between the vocabularies of two word2vec models\n",
    "    \"\"\"\n",
    "\n",
    "    def intersection_align_gensim(self, m1, m2, words=None):\n",
    "        \"\"\"\n",
    "        Intersect two gensim word2vec models, m1 and m2.\n",
    "        Only the shared vocabulary between them is kept.\n",
    "        If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "        Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "        These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "            -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "            -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "        The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the vocab for each model\n",
    "        vocab_m1 = set(m1.wv.index_to_key)\n",
    "        vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "        # Find the common vocabulary\n",
    "        common_vocab = vocab_m1 & vocab_m2\n",
    "        if words: common_vocab &= set(words)\n",
    "\n",
    "        # If no alignment necessary because vocab is identical...\n",
    "        if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "            return (m1,m2)\n",
    "\n",
    "        # Otherwise sort by frequency (summed for both)\n",
    "        common_vocab = list(common_vocab)\n",
    "        common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\") + m2.wv.get_vecattr(w, \"count\"), reverse=True)\n",
    "\n",
    "        # Then for each model...\n",
    "        for m in [m1, m2]:\n",
    "            # Replace old syn0norm array with new one (with common vocab)\n",
    "            indices = [m.wv.key_to_index[w] for w in common_vocab]\n",
    "            old_arr = m.wv.vectors\n",
    "            new_arr = np.array([old_arr[index] for index in indices])\n",
    "            m.wv.vectors = new_arr\n",
    "\n",
    "            # Replace old vocab dictionary with new one (with common vocab)\n",
    "            # and old index2word with new one\n",
    "            new_key_to_index = {}\n",
    "            new_index_to_key = []\n",
    "            for new_index, key in enumerate(common_vocab):\n",
    "                new_key_to_index[key] = new_index\n",
    "                new_index_to_key.append(key)\n",
    "            m.wv.key_to_index = new_key_to_index\n",
    "            m.wv.index_to_key = new_index_to_key\n",
    "            \n",
    "            print(len(m.wv.key_to_index), len(m.wv.vectors))\n",
    "            \n",
    "        return (m1,m2)\n",
    "\n",
    "    \"\"\"Then, I define a function for aligning two spaces with [Orthogonal Procrustes](https://simonensemble.github.io/2018-10/orthogonal-procrustes.html):\"\"\"\n",
    "\n",
    "    def smart_procrustes_align_gensim(self, base_embed, other_embed, words=None):\n",
    "        \"\"\"\n",
    "        Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "        Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "        Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "            \n",
    "        First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "        Then do the alignment on the other_embed model.\n",
    "        Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "        Return other_embed.\n",
    "        If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure vocabulary and indices are aligned\n",
    "        in_base_embed, in_other_embed = self.intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "        # re-filling the normed vectors\n",
    "        in_base_embed.wv.fill_norms(force=True)\n",
    "        in_other_embed.wv.fill_norms(force=True)\n",
    "\n",
    "        # get the (normalized) embedding matrices\n",
    "        base_vecs = in_base_embed.wv.get_normed_vectors()\n",
    "        other_vecs = in_other_embed.wv.get_normed_vectors()\n",
    "\n",
    "        # just a matrix dot product with numpy\n",
    "        m = other_vecs.T.dot(base_vecs) \n",
    "        # SVD method from numpy\n",
    "        u, _, v = np.linalg.svd(m)\n",
    "        # another matrix operation\n",
    "        ortho = u.dot(v) \n",
    "        # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "        other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    \n",
    "        \n",
    "        return other_embed\n",
    "\n",
    "\n",
    "    \"\"\" Measure change\n",
    "    Now we will measure the cosine similarity between the embedding of a word in the first time period and the embedding of the same word in the second time period.\n",
    "    Let's define a function that calculates the semantic change of a word:\n",
    "    \"\"\"\n",
    "\n",
    "    def semantic_change(self, word, model_prev, model_cur):\n",
    "        \"\"\"\n",
    "            The function checks the semantic change in the two word2vec models.\n",
    "        \"\"\"\n",
    "        sc = 1-spatial.distance.cosine(model_prev.wv[word], model_cur.wv[word])\n",
    "\n",
    "        return sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334928be5ed23319e5e6550927c5d4f3c3b4919c511fd9b77eaf79aad6ef04de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
